# ü§ñ Agente de Trading com Reinforcement Learning (Q-Learning)

Este reposit√≥rio cont√©m o Trabalho Final de **Aprendizado por Refor√ßo** do **MBA em Data Science & AI da FIAP (10DTSR)**.

O objetivo foi desenvolver e avaliar um agente de trading automatizado, baseado no algoritmo **Q-Learning**, para gerenciar um portf√≥lio de m√∫ltiplos ativos (VALE3, PETR4, BRFS3). O agente deveria aprender uma pol√≠tica de decis√£o (Comprar, Vender ou Manter) para maximizar o retorno financeiro.

---

##  CHALLENGE O Desafio: RL em Mercados Financeiros

O mercado financeiro apresenta desafios √∫nicos para o Aprendizado por Refor√ßo:
* **Espa√ßo de Estados Cont√≠nuo:** O pre√ßo dos ativos e o saldo da conta podem assumir infinitos valores, tornando uma "Q-Table" tabular (baseada em estados discretos) impratic√°vel ou ineficiente.
* **N√£o-Estacionariedade:** O comportamento do mercado muda ao longo do tempo (ex: crises, booms). Uma pol√≠tica aprendida em um ano pode n√£o funcionar no ano seguinte.
* **Valida√ß√£o:** Um simples *train/test split* n√£o √© v√°lido. √â crucial usar m√©todos que respeitem a ordem temporal dos dados para evitar *lookahead bias* (vazamento de dados do futuro).

## üî¨ Metodologia: Da Prova de Conceito √† Valida√ß√£o Temporal

O projeto foi estruturado em uma s√©rie de experimentos iterativos para tentar superar esses desafios.

### 1. Ambiente de Simula√ß√£o (Custom Gym)
Foi criado um ambiente customizado (`TradingEnv`) usando a biblioteca `gymnasium` (Gym).
* **Estado (State):** Inicialmente `[saldo_em_conta, preco_ativo_1, preco_ativo_2, ...]`.
* **A√ß√£o (Action):** Um √∫nico valor discreto que decodifica uma a√ß√£o para cada ativo (0=Manter, 1=Comprar, 2=Vender). Ex: 3 ativos = 3¬≥ = 27 a√ß√µes poss√≠veis.
* **Recompensa (Reward):** A varia√ß√£o percentual do valor total da carteira a cada passo (dia).

### 2. V1: Q-Learning Tabular Simples
* **Abordagem:** O agente usava apenas o saldo e os pre√ßos atuais como estado.
* **Resultado:** Recompensa m√©dia pr√≥xima de 0.0. O agente n√£o aprendeu uma estrat√©gia lucrativa, como esperado, devido ao espa√ßo de estados ser muito grande e cont√≠nuo.

### 3. V2: Enriquecimento de Estado (Feature Engineering)
Para dar mais contexto ao agente, o estado foi enriquecido com indicadores t√©cnicos:
* M√©dia M√≥vel Curta (5 dias)
* M√©dia M√≥vel Longa (20 dias)
* Momentum (Retorno percentual di√°rio)
* **Resultado:** O Sharpe Ratio melhorou ligeiramente, mas ainda permaneceu pr√≥ximo de 0. O agente continuou sem uma estrat√©gia clara.

### 4. V3: Valida√ß√£o Temporal (Walk-Forward Validation)
Esta foi a etapa crucial. Em vez de treinar em todo o dataset de uma vez, implementamos uma valida√ß√£o cruzada temporal:
1.  **Split 1:** Treina em (2020-2022), Testa em (2023).
2.  **Split 2:** Treina em (2021-2023), Testa em (2024).
3.  **Split 3:** Treina em (2022-2024), Testa em (2025).

* **Resultado:** O agente apresentou **desempenho inconsistente**. Em alguns splits, superou o mercado; em outros, teve um desempenho muito inferior.

### 5. V4: Otimiza√ß√£o com Grid Search
Para garantir que o mau desempenho n√£o era apenas devido a hiperpar√¢metros ruins, combinamos a Valida√ß√£o Temporal (V3) com um **Grid Search** para testar v√°rias combina√ß√µes de `alpha` (taxa de aprendizado), `gamma` (desconto) e `epsilon` (explora√ß√£o).

---

## üìä Conclus√µes Finais

O projeto demonstrou com sucesso as limita√ß√µes do Q-Learning tabular para problemas de trading algor√≠tmico:

1.  **Falha na Generaliza√ß√£o:** A valida√ß√£o temporal provou que o agente n√£o consegue generalizar sua pol√≠tica para novos dados de mercado (per√≠odos de teste). Ele **sofre de *overfitting***, memorizando os padr√µes do per√≠odo de treino, mas falhando em se adaptar a novos regimes de mercado.

2.  **Inefici√™ncia do Q-Learning Tabular:** O espa√ßo de estados do mercado financeiro √© vasto demais. Discretizar os pre√ßos e indicadores em uma Q-Table gigante n√£o √© uma abordagem escal√°vel ou eficaz.

3.  **Resultado Final:** Mesmo ap√≥s a otimiza√ß√£o de hiperpar√¢metros (Grid Search), o **Sharpe Ratio m√©dio permaneceu pr√≥ximo de 0.0**, indicando que a estrat√©gia do agente n√£o oferece retorno ajustado ao risco superior ao do mercado (ou aleat√≥rio).

A valida√ß√£o rigorosa foi essencial: sem a valida√ß√£o temporal, um simples teste em um per√≠odo de treino favor√°vel poderia dar a **falsa impress√£o de sucesso**.

## üí° Pr√≥ximos Passos
A dificuldade encontrada n√£o invalida o uso de RL para trading, mas sim da abordagem *tabular*. Os pr√≥ximos passos l√≥gicos seriam:

* **Deep Q-Networks (DQN):** Substituir a Q-Table por uma Rede Neural. Isso permite que o agente lide com estados cont√≠nuos e encontre padr√µes complexos, generalizando melhor o aprendizado.
* **Engenharia de Recompensa:** Testar fun√ß√µes de recompensa mais sofisticadas (ex: Sharpe Ratio diferencial) em vez de apenas o retorno di√°rio.

## üõ†Ô∏è Stack Tecnol√≥gica
* **Linguagem:** Python
* **Dados:** yfinance (Coleta de dados de mercado)
* **An√°lise:** Pandas, NumPy, Matplotlib
* **Ambiente de RL:** Gymnasium (Custom Env)
* **Modelagem:** Scikit-learn (StandardScaler)

## üë• Autores
* Erika Koyanagui
* Fabio Asnis Campos da Silva
* Lucas Huber Pissaia
* Matheus Raeski
